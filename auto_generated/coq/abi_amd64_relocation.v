(* Generated by Lem from abis/amd64/abi_amd64_relocation.lem. *)

Require Import Arith.
Require Import Bool.
Require Import List.
Require Import String.
Require Import Program.Wf.

Require Import coqharness.

Open Scope nat_scope.
Open Scope string_scope.

(** [abi_amd64_relocation] contains types and definitions relating to ABI
  * specific relocation functionality for the AMD64 ABI.
  *)

Require Import lem_basic_classes.
Require Export lem_basic_classes.

Require Import lem_map.
Require Export lem_map.

Require Import lem_maybe.
Require Export lem_maybe.

Require Import lem_num.
Require Export lem_num.

Require Import lem_string.
Require Export lem_string.


Require Import error.
Require Export error.

Require Import missing_pervasives.
Require Export missing_pervasives.

Require Import lem_assert_extra.
Require Export lem_assert_extra.


Require Import elf_types_native_uint.
Require Export elf_types_native_uint.

Require Import elf_file.
Require Export elf_file.

Require Import elf_header.
Require Export elf_header.

Require Import elf_relocation.
Require Export elf_relocation.

Require Import elf_symbol_table.
Require Export elf_symbol_table.

Require Import memory_image.
Require Export memory_image.


Require Import abi_classes.
Require Export abi_classes.

Require Import abi_utilities.
Require Export abi_utilities.


(** x86-64 relocation types. *)

Definition r_x86_64_none    :  nat :=  0.
Definition r_x86_64_64    :  nat :=  1.
Definition r_x86_64_pc32    :  nat :=  2.
Definition r_x86_64_got32    :  nat :=  3.
Definition r_x86_64_plt32    :  nat :=  4.
Definition r_x86_64_copy    :  nat :=  5.
Definition r_x86_64_glob_dat    :  nat :=  6.
Definition r_x86_64_jump_slot    :  nat :=  7.
Definition r_x86_64_relative    :  nat :=  8.
Definition r_x86_64_gotpcrel    :  nat :=  9.
Definition r_x86_64_32    :  nat :=  10.
Definition r_x86_64_32s    :  nat :=  11.
Definition r_x86_64_16    :  nat :=  12.
Definition r_x86_64_pc16    :  nat :=  13.
Definition r_x86_64_8    :  nat :=  14.
Definition r_x86_64_pc8    :  nat :=  15.
Definition r_x86_64_dtpmod64    :  nat :=  16.
Definition r_x86_64_dtpoff64    :  nat :=  17.
Definition r_x86_64_tpoff64    :  nat :=  18.
Definition r_x86_64_tlsgd    :  nat :=  19.
Definition r_x86_64_tlsld    :  nat :=  20.
Definition r_x86_64_dtpoff32    :  nat :=  21.
Definition r_x86_64_gottpoff    :  nat :=  22.
Definition r_x86_64_tpoff32    :  nat :=  23.
Definition r_x86_64_pc64    :  nat :=  24.
Definition r_x86_64_gotoff64    :  nat :=  25.
Definition r_x86_64_gotpc32    :  nat :=  26.
Definition r_x86_64_size32    :  nat :=  32.
Definition r_x86_64_size64    :  nat :=  33.
Definition r_x86_64_gotpc32_tlsdesc    :  nat :=  34.
Definition r_x86_64_tlsdesc_call    :  nat :=  35.
Definition r_x86_64_tlsdesc    :  nat :=  36.
Definition r_x86_64_irelative    :  nat :=  37.
(* [?]: removed value specification. *)

Definition string_of_amd64_relocation_type  (rel_type1 : nat )  : string := 
  if beq_nat rel_type1 r_x86_64_none then
    "R_X86_64_NONE"
  else if beq_nat rel_type1 r_x86_64_64 then
    "R_X86_64_64"
  else if beq_nat rel_type1 r_x86_64_pc32 then
    "R_X86_64_PC32"
  else if beq_nat rel_type1 r_x86_64_got32 then
    "R_X86_64_GOT32"
  else if beq_nat rel_type1 r_x86_64_plt32 then
    "R_X86_64_PLT32"
  else if beq_nat rel_type1 r_x86_64_copy then
    "R_X86_64_COPY"
  else if beq_nat rel_type1 r_x86_64_glob_dat then
    "R_X86_64_GLOB_DAT"
  else if beq_nat rel_type1 r_x86_64_jump_slot then
    "R_X86_64_JUMP_SLOT"
  else if beq_nat rel_type1 r_x86_64_relative then
    "R_X86_64_RELATIVE"
  else if beq_nat rel_type1 r_x86_64_gotpcrel then
    "R_X86_64_GOTPCREL"
  else if beq_nat rel_type1 r_x86_64_32 then
    "R_X86_64_32"
  else if beq_nat rel_type1 r_x86_64_32s then
    "R_X86_64_32S"
  else if beq_nat rel_type1 r_x86_64_16 then
    "R_X86_64_16"
  else if beq_nat rel_type1 r_x86_64_pc16 then
    "R_X86_64_PC16"
  else if beq_nat rel_type1 r_x86_64_8 then
    "R_X86_64_8"
  else if beq_nat rel_type1 r_x86_64_pc8 then
    "R_X86_64_PC8"
  else if beq_nat rel_type1 r_x86_64_dtpmod64 then
    "R_X86_64_DTPMOD64"
  else if beq_nat rel_type1 r_x86_64_dtpoff64 then
    "R_X86_64_DTPOFF64"
  else if beq_nat rel_type1 r_x86_64_tpoff64 then
    "R_X86_64_TPOFF64"
  else if beq_nat rel_type1 r_x86_64_tlsgd then
    "R_X86_64_TLSGD"
  else if beq_nat rel_type1 r_x86_64_tlsld then
    "R_X86_64_TLSLD"
  else if beq_nat rel_type1 r_x86_64_dtpoff32 then
    "R_X86_64_DTPOFF32"
  else if beq_nat rel_type1 r_x86_64_gottpoff then
    "R_X86_64_GOTTPOFF"
  else if beq_nat rel_type1 r_x86_64_tpoff32 then
    "R_X86_64_TPOFF32"
  else if beq_nat rel_type1 r_x86_64_pc64 then
    "R_X86_64_PC64"
  else if beq_nat rel_type1 r_x86_64_gotoff64 then
    "R_X86_64_GOTOFF64"
  else if beq_nat rel_type1 r_x86_64_gotpc32 then
    "R_X86_64_GOTPC32"
  else if beq_nat rel_type1 r_x86_64_size32 then
    "R_X86_64_SIZE32"
  else if beq_nat rel_type1 r_x86_64_size64 then
    "R_X86_64_SIZE64"
  else if beq_nat rel_type1 r_x86_64_gotpc32_tlsdesc then
    "R_X86_64_GOTPC32_TLSDESC"
  else if beq_nat rel_type1 r_x86_64_tlsdesc_call then
    "R_X86_64_TLSDESC_CALL"
  else if beq_nat rel_type1 r_x86_64_tlsdesc then
    "R_X86_64_TLSDESC"
  else if beq_nat rel_type1 r_x86_64_irelative then
    "R_X86_64_IRELATIVE"
  else
    "Invalid X86_64 relocation".
(* [?]: removed value specification. *)

Definition amd64_reloc {abifeature : Type} `{Ord abifeature} `{AbiFeatureTagEquiv abifeature}  (r : nat )  : (bool *(annotated_memory_image abifeature -> nat  -> symbol_reference_and_reloc_site  -> (nat *(nat  -> Z  -> nat  -> nat )) % type)) % type:=  
    let n2i := (fun (n : nat )=>(Zpred (Zpos (P_of_succ_nat n)))) in 
    let i2n := Zabs_nat in 
    let i2n_signed := fun (width : nat ) => fun (i : Z ) => (
        if int_gteb i((Zpred (Zpos (P_of_succ_nat 0)))) then 
            if int_gteb i (Coq.ZArith.Zpower.Zpower_nat((Zpred (Zpos (P_of_succ_nat 2)))) (Coq.Init.Peano.minus width( 1))) then DAEMON
            else Zabs_nat i
        else 
            (* We manually encode the 2's complement of the negated value *)
            let negated := Zabs_nat ( Coq.ZArith.BinInt.Zminus((Zpred (Zpos (P_of_succ_nat 0)))) i) in 
            let xormask := ( Coq.Init.Peano.minus (nat_power( 2) width)( 1)) in
            let compl := Coq.Init.Peano.plus( 1) (nat_lxor negated xormask)
            in compl
    )
    in
    match ( (string_of_amd64_relocation_type r)) with 
    | "R_X86_64_NONE" =>            (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 0, (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e))))))
    | "R_X86_64_64" =>              (true,  (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 8, (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => i2n ( Coq.ZArith.BinInt.Zplus(n2i s) a)))))))
    | "R_X86_64_PC32" =>            (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 4, (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => Coq.Init.Peano.minus (i2n ( Coq.ZArith.BinInt.Zplus(n2i s) a)) site_addr))))))
    | "R_X86_64_GOT32" =>           (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 4, (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_PLT32" =>           (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 4, (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_COPY" =>            (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => (size_of_copy_reloc img3 rr, (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_GLOB_DAT" =>        (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => (size_of_def rr, (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_JUMP_SLOT" =>       (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 4 (* CHECK *), (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_RELATIVE" =>        (true,  (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 8, (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_GOTPCREL" =>        (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 8 (* CHECK *), (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_32" =>              (true,  (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 4, (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => i2n ( Coq.ZArith.BinInt.Zplus(n2i s) a)))))))
    | "R_X86_64_32S" =>             (true,  (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 4, (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => i2n_signed( 32) ( Coq.ZArith.BinInt.Zplus(n2i s) a)))))))
    | "R_X86_64_16" =>              (true,  (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 2, (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_PC16" =>            (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 2, (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_8" =>               (true,  (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 1, (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_PC8" =>             (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 1, (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_DTPMOD64" =>        (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 8 (* CHECK *), (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_DTPOFF64" =>        (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 8 (* CHECK *), (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_TPOFF64" =>         (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 8 (* CHECK *), (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_TLSGD" =>           (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 8 (* CHECK *), (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_TLSLD" =>           (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 8 (* CHECK *), (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_DTPOFF32" =>        (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 4 (* CHECK *), (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_GOTTPOFF" =>        (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 8 (* CHECK *), (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_TPOFF32" =>         (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 4 (* CHECK *), (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_PC64" =>            (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 8, (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_GOTOFF64" =>        (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 8, (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_GOTPC32" =>         (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 4, (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_SIZE32" =>          (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 4, (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_SIZE64" =>          (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 8, (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_GOTPC32_TLSDESC" => (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 4 (* CHECK *), (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_TLSDESC_CALL" =>    (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 4 (* CHECK *), (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_TLSDESC" =>         (false, (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 8 (* CHECK *), (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | "R_X86_64_IRELATIVE" =>       (true,  (fun (img3 : annotated_memory_image abifeature) => (fun (site_addr : nat ) => (fun (rr : symbol_reference_and_reloc_site ) => ( 8 (* CHECK *), (fun (s : nat ) => fun (a : Z ) => fun (e : nat ) => e)) (* FIXME *)))))
    | _ => DAEMON
end.
(* [?]: removed value specification. *)

Definition abi_amd64_apply_relocation  (rel : elf64_relocation_a ) (val_map1 : fmap (string ) (Z )) (ef : elf64_file )  : error (relocation_frame (elf64_addr ) (Z )):= 
  if is_elf64_relocatable_file(elf64_file_header ef) then
    let rel_type1 := get_elf64_relocation_a_type rel in
    let a_val    := int_of_elf64_sxword(elf64_ra_addend rel) in
      (** No width, No calculation *)
      if beq_nat rel_type1 r_x86_64_none then
        return0 (NoCopy (fmap_empty))
      (** Width: 64 Calculation: S + A *)
      else if beq_nat rel_type1 r_x86_64_64 then
        lookupM "S" val_map1 >>= (fun (s_val : Z ) =>
      	let result := Lift ( Coq.ZArith.BinInt.Zplus s_val a_val) in
      	let addr   :=(elf64_ra_offset rel) in
      		return0 (NoCopy (fmap_add addr (result, I64, CannotFail) fmap_empty)))
      (** Width: 32 Calculation: S + A - P *)
      else if beq_nat rel_type1 r_x86_64_pc32 then
        lookupM "S" val_map1 >>= (fun (s_val : Z ) =>
        lookupM "P" val_map1 >>= (fun (p_val : Z ) =>
      	let result := Lift ( Coq.ZArith.BinInt.Zminus( Coq.ZArith.BinInt.Zplus s_val a_val) p_val) in
      	let addr   :=(elf64_ra_offset rel) in
      		return0 (NoCopy (fmap_add addr (result, I32, CanFail) fmap_empty))))
      (** Width: 32 Calculation: G + A *)
  		else if beq_nat rel_type1 r_x86_64_got32 then
  		  lookupM "G" val_map1 >>= (fun (g_val : Z ) =>
      	let result := Lift ( Coq.ZArith.BinInt.Zplus g_val a_val) in
      	let addr   :=(elf64_ra_offset rel) in
      		return0 (NoCopy (fmap_add addr (result, I32, CanFail) fmap_empty)))
      (** Width: 32 Calculation: L + A - P *)
  		else if beq_nat rel_type1 r_x86_64_plt32 then
  		  lookupM "L" val_map1 >>= (fun (l_val : Z ) =>
  		  lookupM "P" val_map1 >>= (fun (p_val : Z ) =>
  		  let result := Lift ( Coq.ZArith.BinInt.Zminus( Coq.ZArith.BinInt.Zplus l_val a_val) p_val) in
      	let addr   :=(elf64_ra_offset rel) in
      		return0 (NoCopy (fmap_add addr (result, I32, CanFail) fmap_empty))))
    	(** No width, No calculation *)
		  else if beq_nat rel_type1 r_x86_64_copy then
		    return0 Copy
		  (** Width: 64 Calculation: S *)
		  else if beq_nat rel_type1 r_x86_64_glob_dat then
        lookupM "S" val_map1 >>= (fun (s_val : Z ) =>
      	let result := Lift s_val in
      	let addr   :=(elf64_ra_offset rel) in
      		return0 (NoCopy (fmap_add addr (result, I64, CannotFail) fmap_empty)))
		  (** Width: 64 Calculation: S *)
		  else if beq_nat rel_type1 r_x86_64_jump_slot then
        lookupM "S" val_map1 >>= (fun (s_val : Z ) =>
      	let result := Lift s_val in
      	let addr   :=(elf64_ra_offset rel) in
      		return0 (NoCopy (fmap_add addr (result, I64, CannotFail) fmap_empty)))
		  (** Width: 64 Calculation: B + A *)
		  else if beq_nat rel_type1 r_x86_64_relative then
        lookupM "B" val_map1 >>= (fun (b_val : Z ) =>
      	let result := Lift ( Coq.ZArith.BinInt.Zplus b_val a_val) in
      	let addr   :=(elf64_ra_offset rel) in
      		return0 (NoCopy (fmap_add addr (result, I64, CannotFail) fmap_empty)))
		  (** Width: 32 Calculation: G + GOT + A - P *)
		  else if beq_nat rel_type1 r_x86_64_gotpcrel then
        lookupM "G" val_map1 >>= (fun (g_val : Z ) =>
        lookupM "GOT" val_map1 >>= (fun (got_val : Z ) =>
        lookupM "P" val_map1 >>= (fun (p_val : Z ) =>
      	let result := Lift ( Coq.ZArith.BinInt.Zminus( Coq.ZArith.BinInt.Zplus (Coq.ZArith.BinInt.Zplus g_val got_val) a_val) p_val) in
      	let addr   :=(elf64_ra_offset rel) in
      		return0 (NoCopy (fmap_add addr (result, I32, CanFail) fmap_empty)))))
		  (** Width: 32 Calculation: S + A *)
		  else if beq_nat rel_type1 r_x86_64_32 then
        lookupM "S" val_map1 >>= (fun (s_val : Z ) =>
      	let result := Lift ( Coq.ZArith.BinInt.Zplus s_val a_val) in
      	let addr   :=(elf64_ra_offset rel) in
      		return0 (NoCopy (fmap_add addr (result, I32, CanFail) fmap_empty)))
		  (** Width: 32 Calculation: S + A *)
		  else if beq_nat rel_type1 r_x86_64_32s then
        lookupM "S" val_map1 >>= (fun (s_val : Z ) =>
      	let result := Lift ( Coq.ZArith.BinInt.Zplus s_val a_val) in
      	let addr   :=(elf64_ra_offset rel) in
      		return0 (NoCopy (fmap_add addr (result, I32, CanFail) fmap_empty)))
		  (** Width: 16 Calculation: S + A *)
		  else if beq_nat rel_type1 r_x86_64_16 then
        lookupM "S" val_map1 >>= (fun (s_val : Z ) =>
      	let result := Lift ( Coq.ZArith.BinInt.Zplus s_val a_val) in
      	let addr   :=(elf64_ra_offset rel) in
      		return0 (NoCopy (fmap_add addr (result, I16, CanFail) fmap_empty)))
		  (** Width: 16 Calculation: S + A - P *)
		  else if beq_nat rel_type1 r_x86_64_pc16 then
        lookupM "S" val_map1 >>= (fun (s_val : Z ) =>
        lookupM "P" val_map1 >>= (fun (p_val : Z ) =>
      	let result := Lift ( Coq.ZArith.BinInt.Zminus( Coq.ZArith.BinInt.Zplus s_val a_val) p_val) in
      	let addr   :=(elf64_ra_offset rel) in
      		return0 (NoCopy (fmap_add addr (result, I16, CanFail) fmap_empty))))
		  (** Width: 8 Calculation: S + A *)
		  else if beq_nat rel_type1 r_x86_64_8 then
        lookupM "S" val_map1 >>= (fun (s_val : Z ) =>
      	let result := Lift ( Coq.ZArith.BinInt.Zplus s_val a_val) in
      	let addr   :=(elf64_ra_offset rel) in
      		return0 (NoCopy (fmap_add addr (result, I8, CanFail) fmap_empty)))
      (** Width 8: Calculation: S + A - P *)
		  else if beq_nat rel_type1 r_x86_64_pc8 then
        lookupM "S" val_map1 >>= (fun (s_val : Z ) =>
        lookupM "P" val_map1 >>= (fun (p_val : Z ) =>
      	let result := Lift ( Coq.ZArith.BinInt.Zminus( Coq.ZArith.BinInt.Zplus s_val a_val) p_val) in
      	let addr   :=(elf64_ra_offset rel) in
      		return0 (NoCopy (fmap_add addr (result, I8, CanFail) fmap_empty))))
      (** Width: 64 *)
		  else if beq_nat rel_type1 r_x86_64_dtpmod64 then
		    DAEMON
      (** Width: 64 *)
		  else if beq_nat rel_type1 r_x86_64_dtpoff64 then
		    DAEMON
      (** Width: 64 *)
		  else if beq_nat rel_type1 r_x86_64_tpoff64 then
		    DAEMON
      (** Width: 32 *)
		  else if beq_nat rel_type1 r_x86_64_tlsgd then
		    DAEMON
      (** Width: 32 *)
		  else if beq_nat rel_type1 r_x86_64_tlsld then
		    DAEMON
      (** Width: 32 *)
		  else if beq_nat rel_type1 r_x86_64_dtpoff32 then
		    DAEMON
      (** Width: 32 *)
		  else if beq_nat rel_type1 r_x86_64_gottpoff then
		    DAEMON
      (** Width: 32 *)
		  else if beq_nat rel_type1 r_x86_64_tpoff32 then
		    DAEMON
		  (** Width: 64 Calculation: S + A - P *)
		  else if beq_nat rel_type1 r_x86_64_pc64 then
        lookupM "S" val_map1 >>= (fun (s_val : Z ) =>
        lookupM "P" val_map1 >>= (fun (p_val : Z ) =>
      	let result := Lift ( Coq.ZArith.BinInt.Zminus( Coq.ZArith.BinInt.Zplus s_val a_val) p_val) in
      	let addr   :=(elf64_ra_offset rel) in
      		return0 (NoCopy (fmap_add addr (result, I64, CannotFail) fmap_empty))))
		  (** Width: 64 Calculation: S + A - GOT *)
		  else if beq_nat rel_type1 r_x86_64_gotoff64 then
        lookupM "S" val_map1 >>= (fun (s_val : Z ) =>
        lookupM "GOT" val_map1 >>= (fun (got_val : Z ) =>
      	let result := Lift ( Coq.ZArith.BinInt.Zminus( Coq.ZArith.BinInt.Zplus s_val a_val) got_val) in
      	let addr   :=(elf64_ra_offset rel) in
      		return0 (NoCopy (fmap_add addr (result, I64, CannotFail) fmap_empty))))
		  (** Width: 32 Calculation: GOT + A - P *)
		  else if beq_nat rel_type1 r_x86_64_gotpc32 then
        lookupM "GOT" val_map1 >>= (fun (got_val : Z ) =>
        lookupM "P" val_map1 >>= (fun (p_val : Z ) =>
      	let result := Lift ( Coq.ZArith.BinInt.Zminus( Coq.ZArith.BinInt.Zplus got_val a_val) p_val) in
      	let addr   :=(elf64_ra_offset rel) in
      		return0 (NoCopy (fmap_add addr (result, I32, CanFail) fmap_empty))))
		  (** Width: 32 Calculation: Z + A *)
		  else if beq_nat rel_type1 r_x86_64_size32 then
        lookupM "Z" val_map1 >>= (fun (z_val : Z ) =>
      	let result := Lift ( Coq.ZArith.BinInt.Zplus z_val a_val) in
      	let addr   :=(elf64_ra_offset rel) in
      		return0 (NoCopy (fmap_add addr (result, I32, CanFail) fmap_empty)))
		  (** Width: 64 Calculation: Z + A *)
		  else if beq_nat rel_type1 r_x86_64_size64 then
        lookupM "Z" val_map1 >>= (fun (z_val : Z ) =>
      	let result := Lift ( Coq.ZArith.BinInt.Zplus z_val a_val) in
      	let addr   :=(elf64_ra_offset rel) in
      		return0 (NoCopy (fmap_add addr (result, I64, CannotFail) fmap_empty)))
      (** Width: 32 *)
		  else if beq_nat rel_type1 r_x86_64_gotpc32_tlsdesc then
		    DAEMON
      (** No width *)
		  else if beq_nat rel_type1 r_x86_64_tlsdesc_call then
		    DAEMON
		  (** Width: 64X2 *)
		  else if beq_nat rel_type1 r_x86_64_tlsdesc then
		    DAEMON
		  (** Calculation: indirect(B + A) *)
		  else if beq_nat rel_type1 r_x86_64_irelative then
        lookupM "B" val_map1 >>= (fun (b_val : Z ) =>
		    let result := Apply(Indirect, Lift( Coq.ZArith.BinInt.Zplus b_val a_val)) in
		    let addr   :=(elf64_ra_offset rel) in
		      return0 (NoCopy (fmap_add addr (result, I64, CannotFail) fmap_empty)))
		  else
		  	DAEMON
  else
  	DAEMON.
